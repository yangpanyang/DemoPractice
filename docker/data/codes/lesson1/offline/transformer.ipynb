{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bit7a2de7c453e84dbe810d3a0638a5278c",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0.22378181,  0.04090982, -1.17047502],\n",
       "       [ 1.608533  , -0.3095742 , -0.27756728],\n",
       "       [-0.00699097, -0.62198901, -0.72033624],\n",
       "       [ 0.45114196, -0.14505998,  0.35111736]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.normal(0, 1, [4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(position, d_model):\n",
    "    x = np.random.normal(0, 1, [position, d_model])\n",
    "    x[:, 0::2] = np.sin(x[:, 0::2])\n",
    "    x[:, 1::2] = np.cos(x[:, 1::2])\n",
    "    return tf.cast([np.newaxis, ...], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "max_position_encoding = 1000\n",
    "\n",
    "# From FR to EN\n",
    "input_vocab_size = 50000 + 2  # FR\n",
    "target_vocab_size = 50000 + 2  # EN\n",
    "\n",
    "# Encoder\n",
    "\n",
    "# 1.Embedding\n",
    "input = tf.kears.layers.Input(shape=(None, ))\n",
    "x = tf.keras.layers.Embedding(input_vocab_size, d_model)(input)\n",
    "\n",
    "# 2.Position Encoding\n",
    "pos = position_encoding(max_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[:, :tf.shape(x)[1], :]])\n",
    "\n",
    "# 3.Attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)  # dense = Q Matrix\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])  # 区别\n",
    "x = tf.keras.layers.Add()([x, attention])\n",
    "x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "# 4.FNN\n",
    "dense = tf.keras.layers.Dense(d_model, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x, dense])\n",
    "encoder = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# 5.Embedding\n",
    "target = tf.kears.layers.Input(shape=(None, ))\n",
    "x = tf.keras.layers.Embedding(target_vocab_size, d_model)(target)\n",
    "\n",
    "# 6.Position Encoding\n",
    "pos = position_encoding(max_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[:, :tf.shape(x)[1], :]])\n",
    "\n",
    "# 7.Self-Attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)  # dense = Q Matrix\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])\n",
    "x = tf.keras.layers.Add()([x, attention])\n",
    "x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "# 8.Encoder-Decoder Attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)  # dense = Q Matrix\n",
    "value = tf.keras.layers.Dense(d_model)(encoder)\n",
    "key = tf.keras.layers.Dense(d_model)(encoder)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])  # 区别\n",
    "x = tf.keras.layers.Add()([x, attention])\n",
    "x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "# 9.FNN\n",
    "dense = tf.keras.layers.Dense(d_model, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x, dense])\n",
    "decoder = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "# 10.final dense\n",
    "x = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "base_model = tf.keras.model.Model(inputs=[input, target], outputs=x)\n",
    "\n",
    "# 11.optimizer, loss, metrics\n",
    "optimizer = tf.keras.optimizer.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossEntropy(from_logits=True, reduction='none')\n",
    "metrics = []\n",
    "\n",
    "# 12.model compile\n",
    "base_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# 13.training\n",
    "base_model.fit(x = [input, target[:, :-1]], y = target[:, 1:])\n",
    "\n",
    "# 14.predictions\n",
    "result = [target_vocab_size - 2]\n",
    "for _ in range(40):\n",
    "    predict_result = base_model.predict([input, np.asarray(result)])\n",
    "    result.append(np.argmax(predict_result[-1, -1]))\n",
    "    if result[-1] == target_vocab_size -1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = tf.keras.layers.Input(shape=(None, ))\n",
    "target = tf.keras.layers.Input(shape=(None, ))\n",
    "\n",
    "encoder = Encoder(...)\n",
    "decoder = Decoder(...)\n",
    "\n",
    "x = encoder(input)\n",
    "x = decoder([target, x])\n",
    "x = tf.keras.layers.Dense(target_vocab_size)(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model % num_heads == 0, \"the number of heads must be divided by model emb size!\"\n",
    "        depth = d_model // num_heads\n",
    "\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_head, depth)\n",
    "        self.w_query = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        self.split_reshape_query = tf.keras.layers.Reshape((-1, num_head, depth))  # (batch_size, seq_len, num_head, depth)\n",
    "        self.split_permute_query = tf.keras.layers.Permute((2, 1, 3)) # (batch_size, num_head, seq_len, depth)\n",
    "\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_head, depth)\n",
    "        self.w_value = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        self.split_reshape_value = tf.keras.layers.Reshape((-1, num_head, depth))  # (batch_size, seq_len, num_head, depth)\n",
    "        self.split_permute_value = tf.keras.layers.Permute((2, 1, 3)) # (batch_size, num_head, seq_len, depth)\n",
    "\n",
    "        # (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_head, depth)\n",
    "        self.w_key = tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "        self.split_reshape_key = tf.keras.layers.Reshape((-1, num_head, depth))  # (batch_size, seq_len, num_head, depth)\n",
    "        self.split_permute_key = tf.keras.layers.Permute((2, 1, 3)) # (batch_size, num_head, seq_len, depth)\n",
    "\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        self.join_permute = tf.keras.layers.Permute((2, 1, 3))\n",
    "        self.join_reshape = tf.keras.layers.Reshape((-1, d_model))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        q, v, k = inputs\n",
    "\n",
    "        query = self.split_permute_query(self.split_reshape_query(self.w_query(q)))\n",
    "        value = self.split_permute_value(self.split_reshape_value(self.w_value(v)))\n",
    "        key = self.split_permute_key(self.split_reshape_key(self.w_key(k)))\n",
    "\n",
    "        attention = self.attention([query, value, key], mask=True)\n",
    "        attention = self.join_reshape(self.join_permute(attention))\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layers):\n",
    "    def __init__():\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model, mask_zero=True)\n",
    "        self.multi_head_attention = MultiHeadAttention\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        embedding_mask = x.embedding.compute_mask(inputs)\n",
    "        x = self.multi_head_attention(x, mask=embedding_mask)\n"
   ]
  }
 ]
}